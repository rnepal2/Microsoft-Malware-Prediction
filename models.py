# @author: Rabindra Nepal

import os
import pandas as pd
import lightgbm
from lightgbm import LGBMClassifier

from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_validate
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,train_test_split
from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score


class Classifier:
    
    def __init__(self):
        pass
    
    def model(self):
        model = LGBMClassifier(
            n_estimators=300,
            num_leaves=100,
            colsample_bytree=.8,
            subsample=.8,
            max_depth=20,
            reg_alpha=.1,
            reg_lambda=.1,
            min_split_gain=.01, 
            silent=False,
            )
        return model
    
    def encode(self, df):
        # encoding the categorical variables
        encoder = LabelEncoder()
        for column in df.columns.values:
            try:
                if df[column].dtype == pd.api.types.CategoricalDtype():
                    df[column] = encoder.fit_transform(df[column])
                    print('%s is encoded.' % column)
            except:
                continue
        return df
    
    def train(self, model, train_df):
        y_train = train_df['HasDetections'].values
        train_labels = list(train_df.columns.values)
        train_labels.remove('HasDetections')
        x_train = train_df[list(train_labels)]
        # fitting the model
        model.fit(x_train, y_train)
        return model
    
    def predict(self, model, test_df):
        pred = model.predict(test_df)
        return pred


params = {'num_leaves': 300,
          'min_data_in_leaf': 10,
          'objective': 'binary',
          'max_depth': -1,
          'learning_rate': 0.05,
          "boosting": "gbdt",
          "feature_fraction": 0.7,
          "bagging_freq": 1,
          "bagging_fraction": 0.8,
          "bagging_seed": 15,
          "metric": 'auc',
          "lambda_l1": 0.1,
          "random_state": 2019,
          "verbosity": 1,
         }


class Model:
    def __init__(self, train_df, test_df):
	    self.train_df = train_df
	    self.test_df = test_df
    
    def train(self, model, params, boosting_iterations, verbose_eval=None, early_stopping=100):
        '''
		trains the model - selected on the train data splitting into train and validation datasets.
		parameter:
			model = string: model name = 'lgb-train' or 'lgb-cv'
			params = dict: parameters of the model
			boosting_iterations: int: num_boost_round
			verbose_eval = bool: True or False
			early_stopping = int: early_stopping_rounds
		'''
        
        if model not in ['lgb-train', 'lgb-cv']: raise ValueError('Either lgb-train or lgb-cv allowed.')
        
        # train/val data 
        ys = self.train_df['HasDetections']
        Xs = self.train_df.drop(columns={'HasDetections'})
        X_train, X_test, y_train, y_test = train_test_split(Xs, ys, test_size=0.2, random_state=19)
 
		# SOLVE: y_train = target column of the train_df
		# y_train = self.train_df['HasDetections'] => train_df = train_df.drop('HasDetections')
        train_data = lightgbm.Dataset(X_train[:400000], label=y_train[:400000])
        val_data = lightgbm.Dataset(X_test[400000:], label=y_test[400000:])
        
        if model == 'lgb-train':
            
            if verbose_eval:
                verbose_eval = True
            else:
                verbose_eval = False
            
            lightgbm.train(params, train_data, num_boost_round= boosting_iterations, valid_sets = [train_data, val_data], 
                           verbose_eval= verbose_eval, early_stopping_rounds=200)
            
            print('LightGBM model training completed!')
        
        if model == 'lgb-cv':
            
            if verbose_eval:
                verbose_eval = True
            else:
                verbose_eval = False
            
            lightgbm.cv(params, train_data, num_boost_round= boosting_iterations,verbose_eval=verbose_eval, 
                        early_stopping_rounds=early_stopping)
            
            print('LightGBM model training with CV completed!')
            
        
        def predict(self):
            return lightgbm.predict(self.test_df.values)






